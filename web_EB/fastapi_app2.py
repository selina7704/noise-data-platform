# import os
# import shutil
# import librosa
# import numpy as np
# import tensorflow as tf 
# import streamlit as st
# import io
# from fastapi import FastAPI, File, UploadFile
# from fastapi.responses import HTMLResponse
# from fastapi.templating import Jinja2Templates
# from fastapi import Request
# from tensorflow.keras.models import load_model
# import logging 


# app = FastAPI()

# # GPU ë¹„í™œì„±í™” (CPUë¡œë§Œ ì‹¤í–‰)
# tf.config.set_visible_devices([], 'GPU')

# # resnet ëª¨ë¸ ë¡œë“œ 
# model = tf.keras.models.load_model('../web/resnet_model_modified_v6.h5')       

# # ğŸ”¹ ì†ŒìŒ ìœ í˜•ë³„ ë°ì‹œë²¨ ê¸°ì¤€ ì„¤ì •
# SPL_REFERENCE = 20e-6  # 0dB ê¸°ì¤€ ìŒì••
# DB_REFERENCE = {
#     "ì°¨ëŸ‰ ê²½ì ": 100,
#     "ì´ë¥œì°¨ ê²½ì ": 100,
#     "ì‚¬ì´ë Œ": 100,
#     "ì°¨ëŸ‰ ì£¼í–‰ìŒ": 90,
#     "ì´ë¥œì°¨ ì£¼í–‰ìŒ": 90,
#     "ê¸°íƒ€ ì†ŒìŒ": 85,
# }


# # ğŸ”¹ ì˜¤ë””ì˜¤ ë¶„ì„ í•¨ìˆ˜
# def analyze_audio(file_bytes, predicted_label):
#     try:
#         y, sr = librosa.load(io.BytesIO(file_bytes), sr=None, mono=False)
        
#         if y is None or len(y) == 0:
#             logging.error("âŒ librosaê°€ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í•¨!")
#             return {"error": "librosaê°€ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í•¨"}
        
#         is_stereo = len(y.shape) == 2 and y.shape[0] == 2

#         if is_stereo:
#             left_channel = y[0]
#             right_channel = y[1]
#             rms_total = np.sqrt(np.mean((left_channel + right_channel) ** 2)) / 2
#         else:
#             rms_total = np.sqrt(np.mean(y ** 2))

#         if rms_total == 0:
#             logging.error("âŒ RMS ê³„ì‚° ì¤‘ ê°’ì´ 0ì´ ë¨!")
#             return {"error": "RMS ê³„ì‚° ì˜¤ë¥˜"}

#         rms_spl = 20 * np.log10(rms_total / SPL_REFERENCE + 1e-6)
#         peak_amplitude = np.max(np.abs(y))
#         peak_spl = 20 * np.log10(peak_amplitude / SPL_REFERENCE + 1e-6)

#         spl_used = peak_spl if predicted_label in ["ì°¨ëŸ‰ ê²½ì ", "ì´ë¥œì°¨ ê²½ì ", "ì‚¬ì´ë Œ"] else rms_spl
#         db_ref = DB_REFERENCE.get(predicted_label, 85)
#         estimated_distance = round(1 * (10 ** ((db_ref - spl_used) / 20)), 2)
#         estimated_distance = max(0.1, min(estimated_distance, 1000))

#         direction = "ì•Œ ìˆ˜ ì—†ìŒ"
#         if is_stereo:
#             rms_left = np.sqrt(np.mean(left_channel ** 2))
#             rms_right = np.sqrt(np.mean(right_channel ** 2))
#             spl_left = 20 * np.log10(rms_left / SPL_REFERENCE + 1e-6)
#             spl_right = 20 * np.log10(rms_right / SPL_REFERENCE + 1e-6)
#             db_difference = spl_left - spl_right
#             if abs(db_difference) < 1.5:
#                 direction = "ì¤‘ì•™"
#             elif 1.5 <= abs(db_difference) < 3:
#                 direction = "ì•½ê°„ ì™¼ìª½" if db_difference > 0 else "ì•½ê°„ ì˜¤ë¥¸ìª½"
#             else:
#                 direction = "ì™¼ìª½" if db_difference > 0 else "ì˜¤ë¥¸ìª½"
        
#         return {
#             "prediction": predicted_label,
#             "spl": round(spl_used, 2),
#             "estimated_distance": estimated_distance,
#             "direction": direction,
#         }
#     except Exception as e:
#         logging.error(f"âŒ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
#         return {"error": str(e)}



# @app.post("/predict/")
# async def predict(file: UploadFile = File(...)):

#     file_bytes = await file.read() # ì—…ë¡œë“œëœ íŒŒì¼ì„ ë°”ì´íŠ¸ë¡œ ì½ì–´ì˜¤ê¸°
#     # ë””ë²„ê¹…
#     print(f"íŒŒì¼ ì´ë¦„: {file.filename}")

#     # BytesIOë¡œ ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ íŒŒì¼ì²˜ëŸ¼ ì½ê¸°

#     audio_bytes = io.BytesIO(file_bytes)
    
#     # librosaë¥¼ ì‚¬ìš©í•´ WAV íŒŒì¼ì„ ë¦¬ìƒ˜í”Œë§
#     audio_librosa, sr_librosa = librosa.load(audio_bytes, sr=None)
#     print("librosaë¡œ ì²˜ë¦¬í•œ ìƒ˜í”Œë§ ë ˆì´íŠ¸:", sr_librosa)
    
#     mfccs = librosa.feature.mfcc(y=audio_librosa, sr=sr_librosa, n_mfcc=50) 
#     features = np.mean(mfccs, axis=1).astype(float)            
#     print(features)
    
#     # ëª¨ë¸ ì˜ˆì¸¡
#     prediction = model.predict(np.array([features]))  
#     predicted_label = np.argmax(prediction)  

#     # ì†ŒìŒ ì¢…ë¥˜ ë¼ë²¨
#     noise_labels = ['ì´ë¥œì°¨ê²½ì ', 'ì´ë¥œì°¨ì£¼í–‰ìŒ', 'ì°¨ëŸ‰ê²½ì ', 'ì°¨ëŸ‰ì‚¬ì´ë Œ', 'ì°¨ëŸ‰ì£¼í–‰ìŒ', 'ê¸°íƒ€ì†ŒìŒ']
#     detected_noise = noise_labels[predicted_label] 

#     print(f"ì˜ˆì¸¡ëœ ì†ŒìŒ ìœ í˜•: {detected_noise}")  # í„°ë¯¸ë„ì— ì¶œë ¥
#     result = analyze_audio(file_bytes,detected_noise)
#     print(result)
#     return result 
#     #return {"prediction": detected_noise}

import os
import shutil
import librosa
import numpy as np
import tensorflow as tf 
import streamlit as st
import io
import matplotlib.pyplot as plt
from fastapi import FastAPI, File, UploadFile
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from fastapi import Request
from tensorflow.keras.models import load_model
import logging
from fastapi.responses import JSONResponse

app = FastAPI()

# GPU ë¹„í™œì„±í™” (CPUë¡œë§Œ ì‹¤í–‰)
tf.config.set_visible_devices([], 'GPU')

# resnet ëª¨ë¸ ë¡œë“œ 
model = tf.keras.models.load_model('../web/resnet_model_modified_v6.h5')

# ì†ŒìŒ ìœ í˜•ë³„ ë°ì‹œë²¨ ê¸°ì¤€ ì„¤ì •
SPL_REFERENCE = 20e-6
DB_REFERENCE = {
    "ì°¨ëŸ‰ ê²½ì ": 100,
    "ì´ë¥œì°¨ ê²½ì ": 100,
    "ì‚¬ì´ë Œ": 100,
    "ì°¨ëŸ‰ ì£¼í–‰ìŒ": 90,
    "ì´ë¥œì°¨ ì£¼í–‰ìŒ": 90,
    "ê¸°íƒ€ ì†ŒìŒ": 85,
}

def analyze_audio(file_bytes, predicted_label):
    try:
        y, sr = librosa.load(io.BytesIO(file_bytes), sr=None, mono=False)
        if y is None or len(y) == 0:
            logging.error("librosaê°€ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í•¨!")
            return {"error": "librosaê°€ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í•¨"}

        is_stereo = len(y.shape) == 2 and y.shape[0] == 2
        if is_stereo:
            left_channel, right_channel = y[0], y[1]
            rms_total = np.sqrt(np.mean((left_channel + right_channel) ** 2)) / 2
        else:
            rms_total = np.sqrt(np.mean(y ** 2))

        rms_spl = 20 * np.log10(rms_total / SPL_REFERENCE + 1e-6)
        peak_amplitude = np.max(np.abs(y))
        peak_spl = 20 * np.log10(peak_amplitude / SPL_REFERENCE + 1e-6)
        
        spl_used = peak_spl if predicted_label in ["ì°¨ëŸ‰ ê²½ì ", "ì´ë¥œì°¨ ê²½ì ", "ì‚¬ì´ë Œ"] else rms_spl
        db_ref = DB_REFERENCE.get(predicted_label, 85)
        estimated_distance = round(1 * (10 ** ((db_ref - spl_used) / 20)), 2)
        estimated_distance = max(0.1, min(estimated_distance, 1000))
        
        return {
            "prediction": predicted_label,
            "spl": round(spl_used, 2),
            "estimated_distance": estimated_distance,
        }
    except Exception as e:
        logging.error(f"ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        return {"error": str(e)}

@app.post("/predict/")
async def predict(file: UploadFile = File(...)):
    file_bytes = await file.read()
    audio_bytes = io.BytesIO(file_bytes)
    audio_librosa, sr_librosa = librosa.load(audio_bytes, sr=None)
    mfccs = librosa.feature.mfcc(y=audio_librosa, sr=sr_librosa, n_mfcc=50)
    features = np.mean(mfccs, axis=1).astype(float)
    prediction = model.predict(np.array([features]))
    predicted_label = np.argmax(prediction)
    noise_labels = ['ì´ë¥œì°¨ê²½ì ', 'ì´ë¥œì°¨ì£¼í–‰ìŒ', 'ì°¨ëŸ‰ê²½ì ', 'ì°¨ëŸ‰ì‚¬ì´ë Œ', 'ì°¨ëŸ‰ì£¼í–‰ìŒ', 'ê¸°íƒ€ì†ŒìŒ']
    detected_noise = noise_labels[predicted_label]
    result = analyze_audio(file_bytes, detected_noise)

    # ğŸ”¹ ì‹œê°í™” (ì•ŒëŒ ê¸°ëŠ¥)
    fig, ax = plt.subplots()
    categories = list(DB_REFERENCE.keys())
    values = [DB_REFERENCE[cat] for cat in categories]
    ax.bar(categories, values, color='gray', alpha=0.5, label='ê¸°ì¤€ ë°ì‹œë²¨')
    ax.bar([detected_noise], [result['spl']], color='red', alpha=0.7, label='í˜„ì¬ ì†ŒìŒ')
    ax.set_ylabel("ë°ì‹œë²¨(dB)")
    ax.set_title("ì†ŒìŒ ê°•ë„ ì‹œê°í™”")
    ax.legend()
    
    img_path = "static/noise_chart.png"
    plt.savefig(img_path)
    plt.close()

    return JSONResponse({"result": result, "image_url": img_path})